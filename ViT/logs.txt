12/21/2024 09:03:56 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:03:56 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:03:56 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:03:56 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:03:56 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:03:56 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:03:56 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:04:00 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:04:00 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:04:00 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:04:00 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:04:00 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:04:00 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:04:00 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:05:14 - WARNING - __main__ - Process rank: -1, device: hpu, n_gpu: 0, distributed training: False
12/21/2024 09:06:57 - WARNING - __main__ - Process rank: -1, device: hpu, n_gpu: 0, distributed training: False
12/21/2024 09:07:20 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:20 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:20 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:20 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:20 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:20 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:20 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:31 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:31 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:31 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:31 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:31 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:31 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:07:31 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 0, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:08:54 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:09:07 - INFO - __main__ - classifier: token
hidden_size: 768
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 3072
  num_heads: 12
  num_layers: 12

12/21/2024 09:09:07 - INFO - __main__ - classifier: token
hidden_size: 1024
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 4096
  num_heads: 16
  num_layers: 24

12/21/2024 09:09:07 - INFO - __main__ - Training parameters Namespace(name='imagenet1k_TF', dataset='imagenet1K', model_type='ViT-B_16', teacher_model_type='ViT-L_16', pretrained_dir='checkpoint/ViT-B_16.npz', teacher_pretrained_dir='pretrained_models/ViT-L_16-224.npz', output_dir='output', img_size=224, train_batch_size=64, eval_batch_size=64, eval_every=1000, learning_rate=0.06, weight_decay=0, num_steps=20000, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=0, seed=42, gradient_accumulation_steps=2, support_inaccurate_perf_test=False, loss_scale=0, data_path='/workspace/imagenet', cache_dataset=False, use_hpu=1, resume=None, run_lazy_mode=True, is_autocast=True, maskedkd=True, len_num_keep=98, n_gpu=0, device=device(type='hpu'))
12/21/2024 09:09:07 - INFO - __main__ - Total Parameter: 	86.6M
12/21/2024 09:11:41 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:41 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:41 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:41 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:41 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:41 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:42 - WARNING - __main__ - Process rank: 0, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:42 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:11:54 - INFO - __main__ - classifier: token
hidden_size: 768
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 3072
  num_heads: 12
  num_layers: 12

12/21/2024 09:11:54 - INFO - __main__ - classifier: token
hidden_size: 1024
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 4096
  num_heads: 16
  num_layers: 24

12/21/2024 09:11:54 - INFO - __main__ - Training parameters Namespace(name='imagenet1k_TF', dataset='imagenet1K', model_type='ViT-B_16', teacher_model_type='ViT-L_16', pretrained_dir='checkpoint/ViT-B_16.npz', teacher_pretrained_dir='pretrained_models/ViT-L_16-224.npz', output_dir='output', img_size=224, train_batch_size=64, eval_batch_size=64, eval_every=1000, learning_rate=0.06, weight_decay=0, num_steps=20000, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=0, seed=42, gradient_accumulation_steps=2, support_inaccurate_perf_test=False, loss_scale=0, data_path='/workspace/imagenet', cache_dataset=False, use_hpu=1, resume=None, run_lazy_mode=True, is_autocast=True, maskedkd=True, len_num_keep=98, n_gpu=0, device=device(type='hpu'))
12/21/2024 09:11:54 - INFO - __main__ - Total Parameter: 	86.6M
12/21/2024 09:11:54 - INFO - __main__ - ========    args    ========= 

12/21/2024 09:11:54 - INFO - __main__ - Namespace(name='imagenet1k_TF', dataset='imagenet1K', model_type='ViT-B_16', teacher_model_type='ViT-L_16', pretrained_dir='checkpoint/ViT-B_16.npz', teacher_pretrained_dir='pretrained_models/ViT-L_16-224.npz', output_dir='output', img_size=224, train_batch_size=32, eval_batch_size=64, eval_every=1000, learning_rate=0.06, weight_decay=0, num_steps=20000, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=0, seed=42, gradient_accumulation_steps=2, support_inaccurate_perf_test=False, loss_scale=0, data_path='/workspace/imagenet', cache_dataset=False, use_hpu=1, resume=None, run_lazy_mode=True, is_autocast=True, maskedkd=True, len_num_keep=98, n_gpu=0, device=device(type='hpu'))
12/21/2024 09:11:54 - INFO - __main__ - ========            ========= 

12/21/2024 09:12:01 - INFO - __main__ - ***** Running training *****
12/21/2024 09:12:01 - INFO - __main__ - hpu
12/21/2024 09:12:01 - INFO - __main__ -   Total optimization steps = 20000
12/21/2024 09:12:01 - INFO - __main__ -   Instantaneous batch size (per CPU/HPU) = 32
12/21/2024 09:12:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 512
12/21/2024 09:12:01 - INFO - __main__ -   Gradient Accumulation steps = 2
12/21/2024 09:15:22 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:15:22 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:15:22 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:15:23 - WARNING - __main__ - Process rank: 0, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:15:23 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:15:23 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:15:23 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:15:23 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:28 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:28 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:28 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:28 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:29 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:29 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:29 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:29 - WARNING - __main__ - Process rank: 0, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:18:41 - INFO - __main__ - classifier: token
hidden_size: 768
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 3072
  num_heads: 12
  num_layers: 12

12/21/2024 09:18:41 - INFO - __main__ - classifier: token
hidden_size: 1024
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 4096
  num_heads: 16
  num_layers: 24

12/21/2024 09:18:41 - INFO - __main__ - Training parameters Namespace(name='imagenet1k_TF', dataset='imagenet1K', model_type='ViT-B_16', teacher_model_type='ViT-L_16', pretrained_dir='checkpoint/ViT-B_16.npz', teacher_pretrained_dir='pretrained_models/ViT-L_16-224.npz', output_dir='output', img_size=224, train_batch_size=64, eval_batch_size=64, eval_every=1000, learning_rate=0.06, weight_decay=0, num_steps=20000, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=0, seed=42, gradient_accumulation_steps=2, support_inaccurate_perf_test=False, loss_scale=0, data_path='/workspace/imagenet', cache_dataset=False, use_hpu=1, resume=None, run_lazy_mode=True, is_autocast=True, maskedkd=True, len_num_keep=98, n_gpu=0, device=device(type='hpu'))
12/21/2024 09:18:41 - INFO - __main__ - Total Parameter: 	86.6M
12/21/2024 09:18:41 - INFO - __main__ - ========    args    ========= 

12/21/2024 09:18:41 - INFO - __main__ - Namespace(name='imagenet1k_TF', dataset='imagenet1K', model_type='ViT-B_16', teacher_model_type='ViT-L_16', pretrained_dir='checkpoint/ViT-B_16.npz', teacher_pretrained_dir='pretrained_models/ViT-L_16-224.npz', output_dir='output', img_size=224, train_batch_size=32, eval_batch_size=64, eval_every=1000, learning_rate=0.06, weight_decay=0, num_steps=20000, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=0, seed=42, gradient_accumulation_steps=2, support_inaccurate_perf_test=False, loss_scale=0, data_path='/workspace/imagenet', cache_dataset=False, use_hpu=1, resume=None, run_lazy_mode=True, is_autocast=True, maskedkd=True, len_num_keep=98, n_gpu=0, device=device(type='hpu'))
12/21/2024 09:18:41 - INFO - __main__ - ========            ========= 

12/21/2024 09:18:48 - INFO - __main__ - ***** Running training *****
12/21/2024 09:18:48 - INFO - __main__ - hpu
12/21/2024 09:18:48 - INFO - __main__ -   Total optimization steps = 20000
12/21/2024 09:18:48 - INFO - __main__ -   Instantaneous batch size (per CPU/HPU) = 32
12/21/2024 09:18:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 512
12/21/2024 09:18:48 - INFO - __main__ -   Gradient Accumulation steps = 2
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 0, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:39 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 09:21:52 - INFO - __main__ - classifier: token
hidden_size: 768
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 3072
  num_heads: 12
  num_layers: 12

12/21/2024 09:21:52 - INFO - __main__ - classifier: token
hidden_size: 1024
patches:
  size: !!python/tuple
  - 16
  - 16
representation_size: null
transformer:
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  mlp_dim: 4096
  num_heads: 16
  num_layers: 24

12/21/2024 09:21:52 - INFO - __main__ - Training parameters Namespace(name='imagenet1k_TF', dataset='imagenet1K', model_type='ViT-B_16', teacher_model_type='ViT-L_16', pretrained_dir='checkpoint/ViT-B_16.npz', teacher_pretrained_dir='pretrained_models/ViT-L_16-224.npz', output_dir='output', img_size=224, train_batch_size=64, eval_batch_size=64, eval_every=1000, learning_rate=0.06, weight_decay=0, num_steps=20000, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=0, seed=42, gradient_accumulation_steps=2, support_inaccurate_perf_test=False, loss_scale=0, data_path='/workspace/imagenet', cache_dataset=False, use_hpu=1, resume=None, run_lazy_mode=True, is_autocast=True, maskedkd=True, len_num_keep=98, n_gpu=0, device=device(type='hpu'))
12/21/2024 09:21:52 - INFO - __main__ - Total Parameter: 	86.6M
12/21/2024 09:21:52 - INFO - __main__ - ========    args    ========= 

12/21/2024 09:21:52 - INFO - __main__ - Namespace(name='imagenet1k_TF', dataset='imagenet1K', model_type='ViT-B_16', teacher_model_type='ViT-L_16', pretrained_dir='checkpoint/ViT-B_16.npz', teacher_pretrained_dir='pretrained_models/ViT-L_16-224.npz', output_dir='output', img_size=224, train_batch_size=32, eval_batch_size=64, eval_every=1000, learning_rate=0.06, weight_decay=0, num_steps=20000, decay_type='cosine', warmup_steps=500, max_grad_norm=1.0, local_rank=0, seed=42, gradient_accumulation_steps=2, support_inaccurate_perf_test=False, loss_scale=0, data_path='/workspace/imagenet', cache_dataset=False, use_hpu=1, resume=None, run_lazy_mode=True, is_autocast=True, maskedkd=True, len_num_keep=98, n_gpu=0, device=device(type='hpu'))
12/21/2024 09:21:52 - INFO - __main__ - ========            ========= 

12/21/2024 09:21:59 - INFO - __main__ - ***** Running training *****
12/21/2024 09:21:59 - INFO - __main__ - hpu
12/21/2024 09:21:59 - INFO - __main__ -   Total optimization steps = 20000
12/21/2024 09:21:59 - INFO - __main__ -   Instantaneous batch size (per CPU/HPU) = 32
12/21/2024 09:21:59 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 512
12/21/2024 09:21:59 - INFO - __main__ -   Gradient Accumulation steps = 2
12/21/2024 09:25:53 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:25:53 - INFO - __main__ -   Num steps = 782
12/21/2024 09:25:53 - INFO - __main__ -   Batch size = 64
12/21/2024 09:27:05 - INFO - __main__ - 

12/21/2024 09:27:05 - INFO - __main__ - Validation Results
12/21/2024 09:27:05 - INFO - __main__ - Global Steps: 1000
12/21/2024 09:27:05 - INFO - __main__ - Valid Loss: 0.88953
12/21/2024 09:27:05 - INFO - __main__ - Valid Accuracy: 0.78242
12/21/2024 09:27:05 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 09:30:41 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:30:41 - INFO - __main__ -   Num steps = 782
12/21/2024 09:30:41 - INFO - __main__ -   Batch size = 64
12/21/2024 09:31:51 - INFO - __main__ - 

12/21/2024 09:31:51 - INFO - __main__ - Validation Results
12/21/2024 09:31:51 - INFO - __main__ - Global Steps: 2000
12/21/2024 09:31:51 - INFO - __main__ - Valid Loss: 0.86371
12/21/2024 09:31:51 - INFO - __main__ - Valid Accuracy: 0.78192
12/21/2024 09:35:34 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:35:34 - INFO - __main__ -   Num steps = 782
12/21/2024 09:35:34 - INFO - __main__ -   Batch size = 64
12/21/2024 09:36:44 - INFO - __main__ - 

12/21/2024 09:36:44 - INFO - __main__ - Validation Results
12/21/2024 09:36:44 - INFO - __main__ - Global Steps: 3000
12/21/2024 09:36:44 - INFO - __main__ - Valid Loss: 0.80457
12/21/2024 09:36:44 - INFO - __main__ - Valid Accuracy: 0.79256
12/21/2024 09:36:44 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 09:40:20 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:40:20 - INFO - __main__ -   Num steps = 782
12/21/2024 09:40:20 - INFO - __main__ -   Batch size = 64
12/21/2024 09:41:29 - INFO - __main__ - 

12/21/2024 09:41:29 - INFO - __main__ - Validation Results
12/21/2024 09:41:29 - INFO - __main__ - Global Steps: 4000
12/21/2024 09:41:29 - INFO - __main__ - Valid Loss: 0.81335
12/21/2024 09:41:29 - INFO - __main__ - Valid Accuracy: 0.79120
12/21/2024 09:45:04 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:45:04 - INFO - __main__ -   Num steps = 782
12/21/2024 09:45:04 - INFO - __main__ -   Batch size = 64
12/21/2024 09:46:14 - INFO - __main__ - 

12/21/2024 09:46:14 - INFO - __main__ - Validation Results
12/21/2024 09:46:14 - INFO - __main__ - Global Steps: 5000
12/21/2024 09:46:14 - INFO - __main__ - Valid Loss: 0.80421
12/21/2024 09:46:14 - INFO - __main__ - Valid Accuracy: 0.79164
12/21/2024 09:49:50 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:49:50 - INFO - __main__ -   Num steps = 782
12/21/2024 09:49:50 - INFO - __main__ -   Batch size = 64
12/21/2024 09:51:00 - INFO - __main__ - 

12/21/2024 09:51:00 - INFO - __main__ - Validation Results
12/21/2024 09:51:00 - INFO - __main__ - Global Steps: 6000
12/21/2024 09:51:00 - INFO - __main__ - Valid Loss: 0.77472
12/21/2024 09:51:00 - INFO - __main__ - Valid Accuracy: 0.79972
12/21/2024 09:51:00 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 09:54:37 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:54:37 - INFO - __main__ -   Num steps = 782
12/21/2024 09:54:37 - INFO - __main__ -   Batch size = 64
12/21/2024 09:55:48 - INFO - __main__ - 

12/21/2024 09:55:48 - INFO - __main__ - Validation Results
12/21/2024 09:55:48 - INFO - __main__ - Global Steps: 7000
12/21/2024 09:55:48 - INFO - __main__ - Valid Loss: 0.77089
12/21/2024 09:55:48 - INFO - __main__ - Valid Accuracy: 0.80028
12/21/2024 09:55:48 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 09:59:27 - INFO - __main__ - ***** Running Validation *****
12/21/2024 09:59:27 - INFO - __main__ -   Num steps = 782
12/21/2024 09:59:27 - INFO - __main__ -   Batch size = 64
12/21/2024 10:00:37 - INFO - __main__ - 

12/21/2024 10:00:37 - INFO - __main__ - Validation Results
12/21/2024 10:00:37 - INFO - __main__ - Global Steps: 8000
12/21/2024 10:00:37 - INFO - __main__ - Valid Loss: 0.76970
12/21/2024 10:00:37 - INFO - __main__ - Valid Accuracy: 0.80000
12/21/2024 10:04:14 - INFO - __main__ - ***** Running Validation *****
12/21/2024 10:04:14 - INFO - __main__ -   Num steps = 782
12/21/2024 10:04:14 - INFO - __main__ -   Batch size = 64
12/21/2024 10:05:24 - INFO - __main__ - 

12/21/2024 10:05:24 - INFO - __main__ - Validation Results
12/21/2024 10:05:24 - INFO - __main__ - Global Steps: 9000
12/21/2024 10:05:24 - INFO - __main__ - Valid Loss: 0.76925
12/21/2024 10:05:24 - INFO - __main__ - Valid Accuracy: 0.79852
12/21/2024 10:08:59 - INFO - __main__ - ***** Running Validation *****
12/21/2024 10:08:59 - INFO - __main__ -   Num steps = 782
12/21/2024 10:08:59 - INFO - __main__ -   Batch size = 64
12/21/2024 10:10:09 - INFO - __main__ - 

12/21/2024 10:10:09 - INFO - __main__ - Validation Results
12/21/2024 10:10:09 - INFO - __main__ - Global Steps: 10000
12/21/2024 10:10:09 - INFO - __main__ - Valid Loss: 0.75723
12/21/2024 10:10:09 - INFO - __main__ - Valid Accuracy: 0.80476
12/21/2024 10:10:09 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 10:13:48 - INFO - __main__ - ***** Running Validation *****
12/21/2024 10:13:48 - INFO - __main__ -   Num steps = 782
12/21/2024 10:13:48 - INFO - __main__ -   Batch size = 64
12/21/2024 10:14:58 - INFO - __main__ - 

12/21/2024 10:14:58 - INFO - __main__ - Validation Results
12/21/2024 10:14:58 - INFO - __main__ - Global Steps: 11000
12/21/2024 10:14:58 - INFO - __main__ - Valid Loss: 0.74395
12/21/2024 10:14:58 - INFO - __main__ - Valid Accuracy: 0.80728
12/21/2024 10:14:58 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 10:18:38 - INFO - __main__ - ***** Running Validation *****
12/21/2024 10:18:38 - INFO - __main__ -   Num steps = 782
12/21/2024 10:18:38 - INFO - __main__ -   Batch size = 64
12/21/2024 10:19:49 - INFO - __main__ - 

12/21/2024 10:19:49 - INFO - __main__ - Validation Results
12/21/2024 10:19:49 - INFO - __main__ - Global Steps: 12000
12/21/2024 10:19:49 - INFO - __main__ - Valid Loss: 0.73590
12/21/2024 10:19:49 - INFO - __main__ - Valid Accuracy: 0.80726
12/21/2024 10:23:23 - INFO - __main__ - ***** Running Validation *****
12/21/2024 10:23:23 - INFO - __main__ -   Num steps = 782
12/21/2024 10:23:23 - INFO - __main__ -   Batch size = 64
12/21/2024 10:24:33 - INFO - __main__ - 

12/21/2024 10:24:33 - INFO - __main__ - Validation Results
12/21/2024 10:24:33 - INFO - __main__ - Global Steps: 13000
12/21/2024 10:24:33 - INFO - __main__ - Valid Loss: 0.73093
12/21/2024 10:24:33 - INFO - __main__ - Valid Accuracy: 0.80864
12/21/2024 10:24:33 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 10:28:10 - INFO - __main__ - ***** Running Validation *****
12/21/2024 10:28:10 - INFO - __main__ -   Num steps = 782
12/21/2024 10:28:10 - INFO - __main__ -   Batch size = 64
12/21/2024 10:29:19 - INFO - __main__ - 

12/21/2024 10:29:19 - INFO - __main__ - Validation Results
12/21/2024 10:29:19 - INFO - __main__ - Global Steps: 14000
12/21/2024 10:29:19 - INFO - __main__ - Valid Loss: 0.73096
12/21/2024 10:29:19 - INFO - __main__ - Valid Accuracy: 0.80958
12/21/2024 10:29:19 - INFO - __main__ - Saved model checkpoint to [DIR: output]
12/21/2024 10:38:47 - WARNING - __main__ - Process rank: 7, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 10:38:47 - WARNING - __main__ - Process rank: 1, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 10:38:47 - WARNING - __main__ - Process rank: 2, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 10:38:47 - WARNING - __main__ - Process rank: 4, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 10:38:47 - WARNING - __main__ - Process rank: 5, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 10:38:47 - WARNING - __main__ - Process rank: 3, device: hpu, n_gpu: 0, distributed training: True
12/21/2024 10:38:47 - WARNING - __main__ - Process rank: 6, device: hpu, n_gpu: 0, distributed training: True
